{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from transformers import GPT2LMHeadModel\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model_hf = GPT2LMHeadModel.from_pretrained(\\\"gpt2\\\") # 124M\\n\",\n",
    "    \"sd_hf = model_hf.state_dict()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for k, v in sd_hf.items():\\n\",\n",
    "    \"    print(k, v.shape)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"sd_hf[\\\"transformer.wpe.weight\\\"].view(-1)[:20]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"%matplotlib inline\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.imshow(sd_hf[\\\"transformer.wpe.weight\\\"], cmap=\\\"gray\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"plt.plot(sd_hf[\\\"transformer.wpe.weight\\\"][:, 150])\\n\",\n",
    "    \"plt.plot(sd_hf[\\\"transformer.wpe.weight\\\"][:, 200])\\n\",\n",
    "    \"plt.plot(sd_hf[\\\"transformer.wpe.weight\\\"][:, 250])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"plt.imshow(sd_hf[\\\"transformer.h.1.attn.c_attn.weight\\\"][:300,:300], cmap=\\\"gray\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from transformers import pipeline, set_seed\\n\",\n",
    "    \"generator = pipeline('text-generation', model='gpt2')\\n\",\n",
    "    \"set_seed(42)\\n\",\n",
    "    \"generator(\\\"Hello, I'm a language model,\\\", max_length=30, num_return_sequences=5)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# let's instead sample manually\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from torch.nn import functional as F\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = GPT2LMHeadModel.from_pretrained(\\\"gpt2\\\") # 124M\\n\",\n",
    "    \"model.eval()\\n\",\n",
    "    \"model.to('cuda')\\n\",\n",
    "    \"torch.manual_seed(42)\\n\",\n",
    "    \"torch.cuda.manual_seed(42)\\n\",\n",
    "    \"tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \\\"Hello, I'm a language model,\\\"\\n\",\n",
    "    \"tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\\n\",\n",
    "    \"tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\\n\",\n",
    "    \"x = tokens.to('cuda')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# generate!\\n\",\n",
    "    \"while x.size(1) < 30: # max_length=30\\n\",\n",
    "    \"    # forward the model to get the logits\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        logits = model(x)[0] # (B, T, vocab_size)\\n\",\n",
    "    \"        # take the logits at the last position\\n\",\n",
    "    \"        logits = logits[:, -1, :] # (B, vocab_size)\\n\",\n",
    "    \"        # get the probabilities\\n\",\n",
    "    \"        probs = F.softmax(logits, dim=-1)\\n\",\n",
    "    \"        # do top-k sampling of 50 (huggingface pipeline default)\\n\",\n",
    "    \"        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\\n\",\n",
    "    \"        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\\n\",\n",
    "    \"        # select a token from the top-k probabilities\\n\",\n",
    "    \"        # note: multinomial does not demand the input to sum to 1\\n\",\n",
    "    \"        ix = torch.multinomial(topk_probs, 1) # (B, 1)\\n\",\n",
    "    \"        # gather the corresponding indices\\n\",\n",
    "    \"        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\\n\",\n",
    "    \"        # append to the sequence\\n\",\n",
    "    \"        x = torch.cat((x, xcol), dim=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# print the generated text\\n\",\n",
    "    \"import tiktoken\\n\",\n",
    "    \"enc = tiktoken.get_encoding('gpt2')\\n\",\n",
    "    \"for i in range(5):\\n\",\n",
    "    \"    tokens = x[i, :30].tolist()\\n\",\n",
    "    \"    decoded = enc.decode(tokens)\\n\",\n",
    "    \"    print(\\\">\\\", decoded)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# tiny shakespeare dataset\\n\",\n",
    "    \"# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\\n\",\n",
    "    \"with open('input.txt', 'r') as f:\\n\",\n",
    "    \"    text = f.read()\\n\",\n",
    "    \"data = text[:1000] # first 1,000 characters\\n\",\n",
    "    \"print(data[:100])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import tiktoken\\n\",\n",
    "    \"enc = tiktoken.get_encoding('gpt2')\\n\",\n",
    "    \"tokens = enc.encode(data)\\n\",\n",
    "    \"print(tokens[:24])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"buf = torch.tensor(tokens[:24 + 1])\\n\",\n",
    "    \"x = buf[:-1].view(4, 6)\\n\",\n",
    "    \"y = buf[1:].view(4, 6)\\n\",\n",
    "    \"print(x)\\n\",\n",
    "    \"print(y)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(sd_hf[\\\"lm_head.weight\\\"].shape)\\n\",\n",
    "    \"print(sd_hf[\\\"transformer.wte.weight\\\"].shape)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"(sd_hf[\\\"lm_head.weight\\\"] == sd_hf[\\\"transformer.wte.weight\\\"]).all()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(sd_hf[\\\"lm_head.weight\\\"].data_ptr())\\n\",\n",
    "    \"print(sd_hf[\\\"transformer.wte.weight\\\"].data_ptr())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# standard deviation grows inside the residual stream\\n\",\n",
    "    \"x = torch.zeros(768)\\n\",\n",
    "    \"n = 100 # e.g. 100 layers\\n\",\n",
    "    \"for i in range(n):\\n\",\n",
    "    \"    x += n**-0.5 * torch.randn(768)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(x.std())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"\\n\",\n",
    "    \"# super simple little MLP\\n\",\n",
    "    \"net = torch.nn.Sequential(\\n\",\n",
    "    \"    torch.nn.Linear(16, 32),\\n\",\n",
    "    \"    torch.nn.GELU(),\\n\",\n",
    "    \"    torch.nn.Linear(32, 1)\\n\",\n",
    "    \")\\n\",\n",
    "    \"torch.random.manual_seed(42)\\n\",\n",
    "    \"x = torch.randn(4, 16)\\n\",\n",
    "    \"y = torch.randn(4, 1)\\n\",\n",
    "    \"net.zero_grad()\\n\",\n",
    "    \"yhat = net(x)\\n\",\n",
    "    \"loss = torch.nn.functional.mse_loss(yhat, y)\\n\",\n",
    "    \"loss.backward()\\n\",\n",
    "    \"print(net[0].weight.grad.view(-1)[:10])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# the loss objective here is (due to readuction='mean')\\n\",\n",
    "    \"# L = 1/4 * [\\n\",\n",
    "    \"#            (y[0] - yhat[0])**2 +\\n\",\n",
    "    \"#            (y[1] - yhat[1])**2 +\\n\",\n",
    "    \"#            (y[2] - yhat[2])**2 +\\n\",\n",
    "    \"#            (y[3] - yhat[3])**2\\n\",\n",
    "    \"#           ]\\n\",\n",
    "    \"# NOTE: 1/4!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# now let's do it with grad_accum_steps of 4, and B=1\\n\",\n",
    "    \"# the loss objective here is different because\\n\",\n",
    "    \"# accumulation in gradient <---> SUM in loss\\n\",\n",
    "    \"# i.e. we instead get:\\n\",\n",
    "    \"# L0 = 1/4(y[0] - yhat[0])**2\\n\",\n",
    "    \"# L1 = 1/4(y[1] - yhat[1])**2\\n\",\n",
    "    \"# L2 = 1/4(y[2] - yhat[2])**2\\n\",\n",
    "    \"# L3 = 1/4(y[3] - yhat[3])**2\\n\",\n",
    "    \"# L = L0 + L1 + L2 + L3\\n\",\n",
    "    \"# NOTE: the \\\"normalizer\\\" of 1/4 is lost\\n\",\n",
    "    \"net.zero_grad()\\n\",\n",
    "    \"for i in range(4):\\n\",\n",
    "    \"    yhat = net(x[i])\\n\",\n",
    "    \"    loss = torch.nn.functional.mse_loss(yhat, y[i])\\n\",\n",
    "    \"    loss = loss / 4 # <-- have to add back the \\\"normalizer\\\"!\\n\",\n",
    "    \"    loss.backward()\\n\",\n",
    "    \"print(net[0].weight.grad.view(-1)[:10])\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# parse and visualize the logfile\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"%matplotlib inline\\n\",\n",
    "    \"\\n\",\n",
    "    \"sz = \\\"124M\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"loss_baseline = {\\n\",\n",
    "    \"    \\\"124M\\\": 3.2924,\\n\",\n",
    "    \"}[sz]\\n\",\n",
    "    \"hella2_baseline = { # HellaSwag for GPT-2\\n\",\n",
    "    \"    \\\"124M\\\": 0.294463,\\n\",\n",
    "    \"    \\\"350M\\\": 0.375224,\\n\",\n",
    "    \"    \\\"774M\\\": 0.431986,\\n\",\n",
    "    \"    \\\"1558M\\\": 0.488946,\\n\",\n",
    "    \"}[sz]\\n\",\n",
    "    \"hella3_baseline = { # HellaSwag for GPT-3\\n\",\n",
    "    \"    \\\"124M\\\": 0.337,\\n\",\n",
    "    \"    \\\"350M\\\": 0.436,\\n\",\n",
    "    \"    \\\"774M\\\": 0.510,\\n\",\n",
    "    \"    \\\"1558M\\\": 0.547,\\n\",\n",
    "    \"}[sz]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# load the log file\\n\",\n",
    "    \"with open(\\\"log124M_40B/log.txt\\\", \\\"r\\\") as f:\\n\",\n",
    "    \"    lines = f.readlines()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# parse the individual lines, group by stream (train,val,hella)\\n\",\n",
    "    \"streams = {}\\n\",\n",
    "    \"for line in lines:\\n\",\n",
    "    \"    step, stream, val = line.strip().split()\\n\",\n",
    "    \"    if stream not in streams:\\n\",\n",
    "    \"        streams[stream] = {}\\n\",\n",
    "    \"    streams[stream][int(step)] = float(val)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# convert each stream from {step: val} to (steps[], vals[])\\n\",\n",
    "    \"# so it's easier for plotting\\n\",\n",
    "    \"streams_xy = {}\\n\",\n",
    "    \"for k, v in streams.items():\\n\",\n",
    "    \"    # get all (step, val) items, sort them\\n\",\n",
    "    \"    xy = sorted(list(v.items()))\\n\",\n",
    "    \"    # unpack the list of tuples to tuple of lists\\n\",\n",
    "    \"    streams_xy[k] = list(zip(*xy))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# create figure\\n\",\n",
    "    \"plt.figure(figsize=(16, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Panel 1: losses: both train and val\\n\",\n",
    "    \"plt.subplot(121)\\n\",\n",
    "    \"xs, ys = streams_xy[\\\"train\\\"] # training loss\\n\",\n",
    "    \"ys = np.array(ys)\\n\",\n",
    "    \"plt.plot(xs, ys, label=f'nanogpt ({sz}) train loss')\\n\",\n",
    "    \"print(\\\"Min Train Loss:\\\", min(ys))\\n\",\n",
    "    \"xs, ys = streams_xy[\\\"val\\\"] # validation loss\\n\",\n",
    "    \"plt.plot(xs, ys, label=f'nanogpt ({sz}) val loss')\\n\",\n",
    "    \"# horizontal line at GPT-2 baseline\\n\",\n",
    "    \"if loss_baseline is not None:\\n\",\n",
    "    \"    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\\\"OpenAI GPT-2 ({sz}) checkpoint val loss\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"steps\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"loss\\\")\\n\",\n",
    "    \"plt.yscale('log')\\n\",\n",
    "    \"plt.ylim(top=4.0)\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.title(\\\"Loss\\\")\\n\",\n",
    "    \"print(\\\"Min Validation Loss:\\\", min(ys))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Panel 2: HellaSwag eval\\n\",\n",
    "    \"plt.subplot(122)\\n\",\n",
    "    \"xs, ys = streams_xy[\\\"hella\\\"] # HellaSwag eval\\n\",\n",
    "    \"ys = np.array(ys)\\n\",\n",
    "    \"plt.plot(xs, ys, label=f\\\"nanogpt ({sz})\\\")\\n\",\n",
    "    \"# horizontal line at GPT-2 baseline\\n\",\n",
    "    \"if hella2_baseline:\\n\",\n",
    "    \"    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\\\"OpenAI GPT-2 ({sz}) checkpoint\\\")\\n\",\n",
    "    \"if hella3_baseline:\\n\",\n",
    "    \"    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\\\"OpenAI GPT-3 ({sz}) checkpoint\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"steps\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"accuracy\\\")\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.title(\\\"HellaSwag eval\\\")\\n\",\n",
    "    \"print(\\\"Max Hellaswag eval:\\\", max(ys))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"base\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.14\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
